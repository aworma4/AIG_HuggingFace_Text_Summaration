{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cb466b4-dfeb-4923-bd23-14b5799d8779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a916080-466e-4088-a041-78a6bd08f701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd2134fc-910e-469a-9fbd-d7f4bc963645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset reddit (C:/Users/AT030915/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae8b63b82bd41dab495afc635b0860c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#doesn't seem to run  for some reason\n",
    "\n",
    "from datasets import load_dataset\n",
    "reddit = load_dataset('reddit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e7c4e2-92df-491d-b519-f5cdbe5a5eb9",
   "metadata": {},
   "source": [
    "# Output saved\n",
    "Selection deleted\n",
    "#doesn't seem to run  for some reason\n",
    "\n",
    "from datasets import load_dataset\n",
    "reddit = load_dataset('reddit')\n",
    "\n",
    "Downloading and preparing dataset reddit/default to C:/Users/AT030915/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e...\n",
    "\n",
    "Downloading data:   0%|          | 0.00/3.14G [00:00<?, ?B/s]\n",
    "\n",
    "Generating train split:   0%|          | 0/3848330 [00:00<?, ? examples/s]\n",
    "\n",
    "Dataset reddit downloaded and prepared to C:/Users/AT030915/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e. Subsequent calls will reuse this data.\n",
    "\n",
    "  0%|          | 0/1 [00:00<?, ?it/s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab173ae-226d-4062-a85b-7bc665d726d9",
   "metadata": {},
   "source": [
    "# Subset the data\n",
    "Want a data set ~ 10k in size \n",
    "Will choose the first 10k league of legends subreddit entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "493cfaa6-b1aa-48dc-98b3-30160159c8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#just loop thorugh taking only the league of legends subreddit data\n",
    "N = 10000\n",
    "S_N = N/10\n",
    "I = 0\n",
    "J = 0\n",
    "subreddit = 'leagueoflegends'\n",
    "data_store = []\n",
    "\n",
    "while(J < N):\n",
    "    data = reddit['train'][I]\n",
    "    data_bool = data['subreddit']\n",
    "    if data_bool == subreddit:\n",
    "        #print(J)\n",
    "        J += 1\n",
    "        data_store.append(data)\n",
    "\n",
    "        if (J % S_N) ==0:\n",
    "            print(J) \n",
    "\n",
    "    I += 1\n",
    "\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21736224-bef8-45eb-9fa8-c8aace732c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_store)\n",
    "df.to_csv(f'data/reddit_{subreddit}_N_{N}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9529c682-0c71-40aa-adc8-a20c361a7593",
   "metadata": {},
   "source": [
    "# Now have a data set\n",
    "Only need the body and summary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "690308d9-960f-4332-ab8b-7ba50b682119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c34ab168-4e2b-48f9-8ccb-9b1639c8c759",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'data/reddit_leagueoflegends_N_10000.csv'\n",
    "df = pd.read_csv(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ca7820d-5342-4783-8b3e-baf1d4a23a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df[0:10]\n",
    "name_small = 'data/reddit_leagueoflegends_N_10.csv'\n",
    "df_small.to_csv(name_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2453b2d7-0154-475a-9c90-5a2558c8bf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/AT030915/.cache/huggingface/datasets/csv/default-1ae881b243be8771/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef3ffb3b97e410297d04c547b4c766c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf8ce5b6fc144e0ae7ddfd460fe06fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31478916ce044be89002ad73d43d8041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/AT030915/.cache/huggingface/datasets/csv/default-1ae881b243be8771/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e8baf071954791ba4a211b8088afe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0.1', 'Unnamed: 0', 'author', 'body', 'normalizedBody', 'subreddit', 'subreddit_id', 'id', 'content', 'summary'],\n",
       "        num_rows: 7\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0.1', 'Unnamed: 0', 'author', 'body', 'normalizedBody', 'subreddit', 'subreddit_id', 'id', 'content', 'summary'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert into data_loader object\n",
    "\n",
    "\n",
    "reddit_lol1 = load_dataset(\"csv\", data_files=name_small)\n",
    "#need to test train spli tas well\n",
    "reddit_lol = reddit_lol1['train'].train_test_split(test_size=0.3)\n",
    "reddit_lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27e07e4b-8abf-4ddd-bc2c-75b67624ed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6d5b356-ff89-426f-9230-a910c9f1f3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"summarize: \" #choose prefix so t5 summarises\n",
    "\n",
    "#define function to \n",
    "def preprocess_function(examples):\n",
    "    main = 'body' #main field name\n",
    "    summary = 'summary' #summary field name \n",
    "    \n",
    "    inputs = [prefix + doc for doc in examples[main]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[summary], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4581e4dd-e3f4-4027-907e-6ae0497336d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60fd4806840c4a4a92e67953c22c15d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4854537d1b634afa9b33b009f87a795f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_reddit_lol = reddit_lol.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe5ca111-9d32-4c87-8ab5-9df0752932b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now use a data collator - which pads the data to the size of the largest entry\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "767779d2-2f2c-4e6e-a978-cd00644d255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now choose an evaluator for the model\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1bb5067-fcc9-432f-90ea-8b76c45b7793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need a function that passes data to the evaulator \n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58b2beee-27cc-4356-9f23-e6cc6c2e81dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now start the transfer learning\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7fbd07aa-490a-41dd-a401-957ceca71a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0.1', 'Unnamed: 0', 'author', 'body', 'normalizedBody', 'subreddit', 'subreddit_id', 'id', 'content', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 7\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0.1', 'Unnamed: 0', 'author', 'body', 'normalizedBody', 'subreddit', 'subreddit_id', 'id', 'content', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_reddit_lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b21b99e-4b3c-456e-8ea8-126679028761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyperparameters now \n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"reddit_lol_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01,\n",
    "    #save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False , #fp16:  set to false for CPU,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_reddit_lol[\"train\"],\n",
    "    eval_dataset=tokenized_reddit_lol[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8cde68a1-3b11-409f-9173-319932ec57f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AT030915\\HuggingFace_Text_Summarisation\\venv\\HF_venv\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:17, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.832088</td>\n",
       "      <td>0.316300</td>\n",
       "      <td>0.237200</td>\n",
       "      <td>0.316300</td>\n",
       "      <td>0.316300</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7, training_loss=0.9907541956220355, metrics={'train_runtime': 18.8336, 'train_samples_per_second': 0.372, 'train_steps_per_second': 0.372, 'total_flos': 680145420288.0, 'train_loss': 0.9907541956220355, 'epoch': 1.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train \n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c2f2d00-34af-4519-80c1-1fc99fddd077",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'reddit_lol_model/'\n",
    "trainer.save_model(file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86a9342-159e-4bd0-836b-8cc2082bb9e9",
   "metadata": {},
   "source": [
    "# Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f66a5ae4-277e-48d6-8d5d-3563d3f0b0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A Reddit post pointed out the critical flaw of freezing top lane at the professional level.\\n\\n\\n\\nTSM's Kayys, a League of Legends Head Scout and Strategy Coach made multiple comments on Fudge's actions in the first minutes of C9's games against TL, creating a disagreement between 2 players at the professional level.\\n\\nAs everyone saw, C9 lost to TL 3 games to 1, and Fudge's lack of pressure in the beginning in the top lane may have contributed a considerable amount, even while playing a top-tier champion.\\n\\nThe main argument lies within the gold distribution. Fudge was able to farm risk-free for the most part, accumulating gold and experience faster than his enemy laner. However, TL gained the same amount, if not more, just spread over 4 or 5 members that balance with Fudge's peaceful farming.\\n\\n\\n\\nKnowledgeable League of Legends commentators mentioned how freezing a lane is more of an individual mindset. It can only work when the entire team synergies and creates a plan for everyone to balance the flow of the rotations. The goal should be to prevent any chaotic team fights without members who choose to freeze.\\n\\nAs the meta of League of Legends fluxuates, freezing follows the same pattern by coming back in style with the changes made in the game's dynamic. This season, hardly any other region favors freezing, instead relying on roaming team fights where objectives are scored. With the additions of new drakes, the soul, and elder buff, playing for objectives has become a much larger part of the game.\\n\\nTower plates also favor constant movement in the lanes since League of Legends players and teams are rewarded for the pressure they apply. Freezing essentially takes away the ability to play for objectives and tower plates by telling the enemy team, 'I'm okay where I am, let me be.'\\n\\nFreezing doesn't fit in League of Legends early on, in many cases among professional players. With such highly coordinated movements, staying idle in the lane hinders the gameplan of many League of Legends Esports teams. While it's unclear if Fudge made these decisions on his own or if his team agreed, the tactic didn't work for C9 as TL rolled over them.\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"summarize: The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes.\"\n",
    "\n",
    "text_lol =\"\"\" They're probably starting Lvmao, former JDG, LNG and most recently BLG sub support. Lele played for RNG youth in the past and was on FPX last split before he got replaced by QiuQiu.\n",
    "Not that many good support players available right now, so probably the best possible pick up for RNG considering budget etc.\"\"\" \n",
    "\n",
    "with open('league_art_2.txt','r') as f:\n",
    "    text_lol_art = f.read()\n",
    "\n",
    "text_lol_art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8448255b-e94e-4fe3-bfa6-6b4ccea4a930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \"a reddit post pointed out the critical flaw of freezing top lane at the professional level . the main argument lies within the gold distribution . TL gained the same amount, if not more, just spread over 4 or 5 members that balance with Fudge's peaceful farming .\"}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=file_path,max_length = 300)\n",
    "summarizer(text_lol_art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1495f647-513e-40c3-939c-f71cd7174699",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
