{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cb466b4-dfeb-4923-bd23-14b5799d8779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a916080-466e-4088-a041-78a6bd08f701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd2134fc-910e-469a-9fbd-d7f4bc963645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset reddit/default to C:/Users/AT030915/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e0ad552cf04cfd9251b410cb692662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.14G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ec274de49d43d58e904b5eb1f1c241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3848330 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset reddit downloaded and prepared to C:/Users/AT030915/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f750d549b4fd4b169f99a136b32a4f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#doesn't seem to run  for some reason\n",
    "\n",
    "from datasets import load_dataset\n",
    "reddit = load_dataset('reddit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e7c4e2-92df-491d-b519-f5cdbe5a5eb9",
   "metadata": {},
   "source": [
    "# Output saved\n",
    "Selection deleted\n",
    "#doesn't seem to run  for some reason\n",
    "\n",
    "from datasets import load_dataset\n",
    "reddit = load_dataset('reddit')\n",
    "\n",
    "Downloading and preparing dataset reddit/default to C:/Users/AT030915/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e...\n",
    "\n",
    "Downloading data:   0%|          | 0.00/3.14G [00:00<?, ?B/s]\n",
    "\n",
    "Generating train split:   0%|          | 0/3848330 [00:00<?, ? examples/s]\n",
    "\n",
    "Dataset reddit downloaded and prepared to C:/Users/AT030915/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e. Subsequent calls will reuse this data.\n",
    "\n",
    "  0%|          | 0/1 [00:00<?, ?it/s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab173ae-226d-4062-a85b-7bc665d726d9",
   "metadata": {},
   "source": [
    "# Subset the data\n",
    "Want a data set ~ 10k in size \n",
    "Will choose the first 10k league of legends subreddit entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "493cfaa6-b1aa-48dc-98b3-30160159c8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "#just loop thorugh taking only the league of legends subreddit data\n",
    "N = 10000\n",
    "S_N = N/10\n",
    "I = 0\n",
    "J = 0\n",
    "subreddit = 'leagueoflegends'\n",
    "data_store = []\n",
    "\n",
    "while(J < N):\n",
    "    data = reddit['train'][I]\n",
    "    data_bool = data['subreddit']\n",
    "    if data_bool == subreddit:\n",
    "        #print(J)\n",
    "        J += 1\n",
    "        data_store.append(data)\n",
    "\n",
    "        if (J % S_N) ==0:\n",
    "            print(J) \n",
    "\n",
    "    I += 1\n",
    "\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "21736224-bef8-45eb-9fa8-c8aace732c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_store)\n",
    "df.to_csv(f'data/reddit_{subreddit}_N_{N}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9529c682-0c71-40aa-adc8-a20c361a7593",
   "metadata": {},
   "source": [
    "# Now have a data set\n",
    "Only need the body and summary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "690308d9-960f-4332-ab8b-7ba50b682119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c34ab168-4e2b-48f9-8ccb-9b1639c8c759",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'data/reddit_leagueoflegends_N_10000.csv'\n",
    "df = pd.read_csv(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2453b2d7-0154-475a-9c90-5a2558c8bf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/AT030915/.cache/huggingface/datasets/csv/default-7267e4e3846f4574/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43754b81a96049f793b6d13bb6d91744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert into data_loader object\n",
    "\n",
    "\n",
    "reddit_lol = load_dataset(\"csv\", data_files=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "27e07e4b-8abf-4ddd-bc2c-75b67624ed7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e939fa0969e840039b3ed942d9714afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AT030915\\HuggingFace_Text_Summarisation\\venv\\HF_venv\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\AT030915\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd105cabb364bad92c095aa9f8408cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e343ca84a745fabb3376c09ad280ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = \"t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c6d5b356-ff89-426f-9230-a910c9f1f3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"summarize: \" #choose prefix so t5 summarises\n",
    "\n",
    "#define function to \n",
    "def preprocess_function(examples):\n",
    "    main = 'body' #main field name\n",
    "    summary = 'summary' #summary field name \n",
    "    \n",
    "    inputs = [prefix + doc for doc in examples[main]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[summary], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4581e4dd-e3f4-4027-907e-6ae0497336d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c749e9a0f2e245f3a61fcdf689abbb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_reddit_lol = reddit_lol.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5ca111-9d32-4c87-8ab5-9df0752932b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
